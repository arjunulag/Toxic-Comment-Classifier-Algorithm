# üõ°Ô∏è Toxic Comment Classifier

A deep learning model for detecting toxic comments using **Transfer Learning** with **DistilBERT**. This project fine-tunes a pre-trained transformer model to classify text as toxic or non-toxic with high accuracy.

---

## üìã Overview

This classifier leverages the power of transfer learning by fine-tuning [DistilBERT](https://huggingface.co/distilbert-base-uncased) (a lighter, faster version of BERT with 66M parameters) on a toxic comments dataset. The model achieves strong performance in identifying harmful, offensive, or toxic language in text.

### Key Features

- **Transfer Learning**: Uses pre-trained DistilBERT for superior language understanding
- **Binary Classification**: Classifies comments as Toxic or Non-Toxic
- **Multiple Interfaces**: CLI tool, interactive mode, and Streamlit web app
- **GPU Support**: Automatically uses CUDA if available for faster inference
- **Production Ready**: Saved model can be easily deployed

---

## üöÄ Quick Start

### Installation

1. Clone the repository and navigate to the project directory

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Download the Dataset

This project uses the **Jigsaw Unintended Bias in Toxicity Classification** dataset from Kaggle.

1. Go to the [Kaggle competition page](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)
2. Download `train.csv`
3. Rename it to `data.csv` and place it in the project root

**Alternative**: Using Kaggle CLI:
```bash
# Install Kaggle CLI (if not already installed)
pip install kaggle

# Download the dataset (requires Kaggle API credentials)
kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification -f train.csv
mv train.csv data.csv
```

> **Note**: You'll need a Kaggle account and API token. See [Kaggle API docs](https://www.kaggle.com/docs/api) for setup.

### Training the Model

Train the model on your dataset:

```bash
python train.py
```

The training script will:
- Load and preprocess `data.csv`
- Fine-tune DistilBERT for 3 epochs
- Save the best model to `toxic_classifier_model/`
- Display training metrics (accuracy, F1, AUC-ROC)

### Making Predictions

**Demo mode** (sample predictions):
```bash
python predict.py --demo
```

**Single prediction**:
```bash
python predict.py --text "Your comment here"
```

**Interactive mode**:
```bash
python predict.py --interactive
```

### Web Application

Launch the Streamlit web interface:
```bash
streamlit run app.py
```

---

## üìÅ Project Structure

```
Classify/
‚îú‚îÄ‚îÄ train.py                    # Training script
‚îú‚îÄ‚îÄ predict.py                  # CLI prediction tool
‚îú‚îÄ‚îÄ app.py                      # Streamlit web application
‚îú‚îÄ‚îÄ data.csv                    # Training dataset (download separately)
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                  # Git ignore rules
‚îú‚îÄ‚îÄ toxic_classifier_model/     # Saved model directory (generated after training)
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îú‚îÄ‚îÄ vocab.txt
‚îÇ   ‚îî‚îÄ‚îÄ special_tokens_map.json
‚îî‚îÄ‚îÄ README.md
```

> **Note**: `data.csv` and `toxic_classifier_model/` are not included in the repository due to size. See download instructions above.

---

## ‚öôÔ∏è Configuration

Training parameters can be modified in the `Config` class within `train.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `MODEL_NAME` | `distilbert-base-uncased` | Pre-trained model to fine-tune |
| `MAX_LENGTH` | `256` | Maximum token sequence length |
| `BATCH_SIZE` | `16` | Training batch size |
| `EPOCHS` | `3` | Number of training epochs |
| `LEARNING_RATE` | `2e-5` | AdamW learning rate |
| `WARMUP_RATIO` | `0.1` | Warmup steps ratio |
| `TOXICITY_THRESHOLD` | `0.5` | Threshold for toxic label |
| `TEST_SIZE` | `0.2` | Validation split ratio |

---

## üìä Dataset Format

The training data (`data.csv`) should contain:

| Column | Description |
|--------|-------------|
| `comment_text` | The text content to classify |
| `target` | Toxicity score (0.0 - 1.0). Values ‚â• 0.5 are labeled as toxic |

---

## üîß Model Architecture

```
DistilBERT (Pre-trained)
    ‚îÇ
    ‚îú‚îÄ‚îÄ 6 Transformer Layers
    ‚îú‚îÄ‚îÄ 768 Hidden Dimensions
    ‚îú‚îÄ‚îÄ 12 Attention Heads
    ‚îÇ
    ‚îî‚îÄ‚îÄ Classification Head (2 classes)
         ‚îú‚îÄ‚îÄ Dropout
         ‚îî‚îÄ‚îÄ Linear Layer ‚Üí [Non-Toxic, Toxic]
```

### Training Process

1. **Tokenization**: Text is tokenized using DistilBERT's WordPiece tokenizer
2. **Fine-tuning**: All model layers are trained with gradient clipping
3. **Optimization**: AdamW optimizer with linear warmup scheduler
4. **Evaluation**: Model selection based on best validation F1 score

---

## üìà Metrics

The model is evaluated using:

- **Accuracy**: Overall classification accuracy
- **Precision**: Fraction of true positives among predicted positives
- **Recall**: Fraction of true positives among actual positives
- **F1 Score**: Harmonic mean of precision and recall
- **AUC-ROC**: Area under the ROC curve

---

## üíª API Usage

### Python Integration

```python
from predict import ToxicCommentClassifier

# Initialize classifier
classifier = ToxicCommentClassifier('toxic_classifier_model')

# Single prediction
result = classifier.predict("This is a test comment")
print(result)
# {
#     'text': 'This is a test comment',
#     'prediction': 'Non-Toxic',
#     'is_toxic': False,
#     'confidence': 0.98,
#     'toxic_probability': 0.02,
#     'non_toxic_probability': 0.98
# }

# Simple classification
label = classifier.classify("Another comment")
print(label)  # 'Non-Toxic' or 'Toxic'

# Batch prediction
results = classifier.predict_batch(["Comment 1", "Comment 2", "Comment 3"])
```

---

## üñ•Ô∏è Web Interface

The Streamlit app (`app.py`) provides:

- Modern, dark-themed UI
- Real-time toxicity analysis
- Confidence scores and probability distribution
- Example buttons for quick testing

---

## üì¶ Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers 4.35+
- CUDA (optional, for GPU acceleration)

See `requirements.txt` for full dependency list.

---

## üî¨ Technical Details

### Tokenization
- **Tokenizer**: DistilBERT WordPiece
- **Max Length**: 256 tokens
- **Padding**: Max length padding
- **Truncation**: Enabled for long texts

### Training
- **Optimizer**: AdamW (Œµ = 1e-8)
- **Scheduler**: Linear warmup + decay
- **Gradient Clipping**: Max norm = 1.0
- **Loss Function**: Cross-entropy

### Model Selection
- Best model is selected based on validation F1 score
- Model weights saved in SafeTensors format

---

## üìù License

This project is for educational and research purposes.

---

## üôè Acknowledgments

- [Hugging Face Transformers](https://huggingface.co/transformers/) for the pre-trained models
- [Jigsaw/Google](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) for the toxic comments dataset
- [Streamlit](https://streamlit.io/) for the web framework
